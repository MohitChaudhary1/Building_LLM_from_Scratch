<h1 align="center">🧠 Building LLMs from Scratch</h1>
<p align="center">
  <img src="assets/llm_architecture.png" alt="LLM Architecture" width="700"/>
</p>

<div align="center">
  <strong>A Deep Dive into the Foundations of Large Language Models</strong><br/>
  <em>Inspired by the book by <a href="https://sebastianraschka.com/">Sebastian Raschka</a></em>
</div>

---

## 📘 About This Repository

This repository is a **hands-on exploration and implementation** of key components required to build a **Large Language Model (LLM)** — **from scratch** — using PyTorch. It’s inspired by the incredible book **“Build a Large Language Model (From Scratch)”** by **Sebastian Raschka**, a renowned ML researcher and educator.

> 🎯 **Purpose:** To learn, practice, and share the core fundamentals of LLMs in an open-source and reproducible manner.

---

## 🌟 What You'll Learn

This project mirrors the structured learning approach of the book while offering runnable notebooks, code explanations, and illustrations. You’ll gain practical insights into:

### 🔤 1. Tokenization
- Implementing a **Byte-Pair Encoding (BPE)** tokenizer
- Understanding how raw text becomes numerical input for LLMs

### 🔡 2. Embeddings & Positional Encoding
- How words are represented in vector space
- Why positional embeddings matter in transformers

### 🔁 3. Self-Attention Mechanism
- Deep dive into **Scaled Dot-Product Attention**
- Multi-head attention visualized and built from scratch

### 🧱 4. Transformer Architecture
- Assembling attention, normalization, feed-forward layers
- Layer stacking and residual connections

### 🧠 5. Training a Causal Language Model
- Custom training loop with gradient clipping
- Loss visualization and perplexity measurement

---

## 📚 Table of Contents

| Notebook |  
|----------|
| Implements and visualizes a BPE tokenizer |
| Word embeddings and positional encodings |
| Scaled dot-product and multi-head attention |
| Complete Transformer block from scratch |
| End-to-end mini LLM training on text |

---
