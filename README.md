<h1 align="center">ğŸ§  Building LLMs from Scratch</h1>
<p align="center">
  <img src="assets/llm_architecture.png" alt="LLM Architecture" width="700"/>
</p>

<div align="center">
  <strong>A Deep Dive into the Foundations of Large Language Models</strong><br/>
  <em>Inspired by the book by <a href="https://sebastianraschka.com/">Sebastian Raschka</a></em>
</div>

---

## ğŸ“˜ About This Repository

This repository is a **hands-on exploration and implementation** of key components required to build a **Large Language Model (LLM)** â€” **from scratch** â€” using PyTorch. Itâ€™s inspired by the incredible book **â€œBuild a Large Language Model (From Scratch)â€** by **Sebastian Raschka**, a renowned ML researcher and educator.

> ğŸ¯ **Purpose:** To learn, practice, and share the core fundamentals of LLMs in an open-source and reproducible manner.

---

## ğŸŒŸ What You'll Learn

This project mirrors the structured learning approach of the book while offering runnable notebooks, code explanations, and illustrations. Youâ€™ll gain practical insights into:

### ğŸ”¤ 1. Tokenization
- Implementing a **Byte-Pair Encoding (BPE)** tokenizer
- Understanding how raw text becomes numerical input for LLMs

### ğŸ”¡ 2. Embeddings & Positional Encoding
- How words are represented in vector space
- Why positional embeddings matter in transformers

### ğŸ” 3. Self-Attention Mechanism
- Deep dive into **Scaled Dot-Product Attention**
- Multi-head attention visualized and built from scratch

### ğŸ§± 4. Transformer Architecture
- Assembling attention, normalization, feed-forward layers
- Layer stacking and residual connections

### ğŸ§  5. Training a Causal Language Model
- Custom training loop with gradient clipping
- Loss visualization and perplexity measurement

---

## ğŸ“š Table of Contents

| Notebook |  
|----------|
| Implements and visualizes a BPE tokenizer |
| Word embeddings and positional encodings |
| Scaled dot-product and multi-head attention |
| Complete Transformer block from scratch |
| End-to-end mini LLM training on text |

---
